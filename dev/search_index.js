var documenterSearchIndex = {"docs":
[{"location":"Boehm/#Getting-started","page":"Getting started","title":"Getting started","text":"","category":"section"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"In this introductory tutorial we will create a PEtabODEproblem for the small Boehm model, while simultaneously covering the main features in PEtab.jl.","category":"page"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"To run the code you need the Boehm PEtab files which can be found here. A fully runnable example of this tutorial can be found here.","category":"page"},{"location":"Boehm/#Reading-a-PEtab-model","page":"Getting started","title":"Reading a PEtab model","text":"","category":"section"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"As a starting point we need to read the PEtab files into Julia. This is made easy by the package which given the path to the PEtab yaml-file PEtab.jl reads all PEtab files into a PEtabModel struct. Here several things happen under the hood:","category":"page"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"The SBML file is translated into ModelingToolkit.jl format (e.g. allow symbolic computation of the ODE-model Jacobian).\nThe observable PEtab table is translated into Julia functions for computing the observables (h), noise parameter (sigma) and initial values (u_0).\nTo be able to compute gradients via adjoint sensitivity analysis and/or forward sensitivity equations the derivatives of h and sigma are computed symbolically with respect to the ODE-models states (u) and parameters.","category":"page"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"All this happens automatically and you can find the resulting files in dirYamlFile/Julia_files/. To save time the function readPEtabModel has the default forceBuildJlFiles=false meaning that the Julia files are not rebuilt in case they already exist.","category":"page"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"using PEtab\nusing OrdinaryDiffEq\nusing Printf\n\npathYaml = joinpath(@__DIR__, \"Boehm\", \"Boehm_JProteomeRes2014.yaml\")\npetabModel = readPEtabModel(pathYaml, verbose=true)","category":"page"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"PEtabModel for model Boehm. ODE-system has 8 states and 10 parameters.\nGenerated Julia files are at ...","category":"page"},{"location":"Boehm/#Creating-a-PEtabODEProblem","page":"Getting started","title":"Creating a PEtabODEProblem","text":"","category":"section"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"Given a PEtab model we can create a PEtabODEProblem (in the future we plan to add surrogate, SDE, etc... problems). The function setupPEtabODEProblem accepts several (full list in API documentation), some key ones are:","category":"page"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"odeSolverOptions - Which ODE solver and which solver tolerances (abstol and reltol). Below we use the ODE solver Rodas5P() (works well for smaller models ≤ 15 states), and we use the default abstol, reltol .= 1e-8.\ngradientMethod - For small models like Boehm forward mode automatic differentiation (AD) is fastest, thus we choose :ForwardDiff.\nhessianMethod - For small models like Boehm with ≤20 parameters it is computationally feasible to compute the full Hessian via forward-mode AD. Thus, we choose :ForwardDiff.","category":"page"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"odeSolverOptions = getODESolverOptions(Rodas5P(), abstol=1e-8, reltol=1e-8)\npetabProblem = setupPEtabODEProblem(petabModel, odeSolverOptions, \n                                    gradientMethod=:ForwardDiff, \n                                    hessianMethod=:ForwardDiff)","category":"page"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"PEtabODEProblem for Boehm. ODE-states: 8. Parameters to estimate: 9 where 6 are dynamic.\n---------- Problem settings ----------\nGradient method : ForwardDiff\nHessian method : ForwardDiff\n--------- ODE-solver settings --------\nCost Rodas5P(). Options (abstol, reltol, maxiters) = (1.0e-08, 1.0e-08, 1.0e+04)\nGradient Rodas5P(). Options (abstol, reltol, maxiters) = (1.0e-08, 1.0e-08, 1.0e+04)","category":"page"},{"location":"Boehm/#Computing-the-cost,-gradient-and-hessian","page":"Getting started","title":"Computing the cost, gradient and hessian","text":"","category":"section"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"The PEtabODEProblem contains everything needed to set up an optimization problem with most available optimizers. Main fields are:","category":"page"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"petabODEProblem.computeCost - Given a parameter vector θ computes the cost (objective function).\npetabODEProblem.computeGradient!- Given a parameter vector θ computes the gradient via chosen method.\npetabODEProblem.computeHessian!- Given a parameter vector θ computes the hessian via chosen method\npetabODEProblem.lowerBounds - A vector with the lower bounds for parameters as specified by the PEtab parameters file.\npetabODEProblem.upperBounds - A vector with the upper bounds for parameters as specified by the PEtab parameters file. \npetabODEProblem.θ_estNames - A vector with the names of the parameters to estimate.","category":"page"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"Note1 - The parameter vector θ is assumed to be on PEtab specified parameter-scale. Thus, if parameter i is on the log-scale so should θ[i] be.\nNote2 - The computeGradient! and computeHessian! functions are in-place functions. Thus, their first argument is an already pre-allocated gradient and hessian respectively (see below).","category":"page"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"# Parameters on log-scale\np = petabProblem.θ_nominalT \ngradient = zeros(length(p))\nhessian = zeros(length(p), length(p))\ncost = petabProblem.computeCost(p)\npetabProblem.computeGradient!(gradient, p)\npetabProblem.computeHessian!(hessian, p)\n@printf(\"Cost = %.2f\\n\", cost)\n@printf(\"First element in the gradient = %.2e\\n\", gradient[1])\n@printf(\"First element in the hessian = %.2f\\n\", hessian[1, 1])","category":"page"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"Cost = 138.22\nFirst element in the gradient = 2.20e-02\nFirst element in the hessian = 2199.49","category":"page"},{"location":"Boehm/#Where-to-go-from-here","page":"Getting started","title":"Where to go from here","text":"","category":"section"},{"location":"Boehm/","page":"Getting started","title":"Getting started","text":"The best create a PEtabODEProblem we recommend looking at Choosing best options for a PEtab problem. We also recommend looking at Supported gradient and hessian methods.","category":"page"},{"location":"Parameter_estimation/#Optimization-(parameter-estimation)","page":"Parameter estimation","title":"Optimization (parameter estimation)","text":"","category":"section"},{"location":"Parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"PEtab.jl is written to easily integrate with available optimization packages such as Optim.jl, Ipopt.jl and Fides.py. In the examples we show how to use these together with PEtab.jl.","category":"page"},{"location":"Parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Based on an extensive benchmark a good rule of thumb when choosing optimizer is:","category":"page"},{"location":"Parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"If you can provide a full Hessian the Interior-point Newton method in Optim.jl often outperforms the trust-region method in Fides.py.\nIn case you cannot provide the full Hessian but the Gauss-Newton hessian approximation the Newton trust-region method in Fides.py often outperforms the interior-point method in Optim.jl.","category":"page"},{"location":"Parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"note: Note\nEvery problem is unique, and the recommended choice here often work well but might not optimal for a specific model","category":"page"},{"location":"Best_options/#best_options","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"","category":"section"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"PEtab.jl supports several gradient and hessian methods. In addition, it is compatible with the ODE solvers in the DifferentialEquations.jl package, hence, there are many possible choices when creating a PEtabODEProblem via setupPEtabODEProblem. To help navigate these based on an extensive benchmark study we here provide recommended settings that often work well for specific problem types.","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"note: Note\nEvery problem is unique, and the recommended settings here often work well but might not optimal for a specific model.","category":"page"},{"location":"Best_options/#Small-models-(\\leq-20-parameters-and-\\leq-15-ODE:s)","page":"Choosing the best options for a PEtab problem","title":"Small models (leq 20 parameters and leq 15 ODE:s)","text":"","category":"section"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"ODE solver: For small stiff models the Rosenbrock Rodas5P() solver is often one of the fastest and most accurate ODE solvers. Julia bdf-solvers such as QNDF() can also perform well here, but they are often less accurate and reliable (fail more often) than Rodas5P(). If the model is “mildly” stiff composite solvers such as AutoVern7(Rodas5P()) often performs best. Regardless of solver we recommend to use low tolerances (around abstol, reltol = 1e-8, 1e-8) to obtain accurate gradients.","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"Gradient method: For small models forward-mode automatic differentiation via ForwardDiff.jl performs best, and is often twice as fast as the forward-sensitivity equations approach in AMICI. Thus, we recommend gradientMethod=:ForwardDiff.","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"Note1 - For :ForwardDiff the user can set the chunk-size. This can substantially improve performance, and we plan to add automatic tuning of it.\nNote2 - If the model has many simulation condition specific parameters (parameters that only appear in a subset of simulation conditions) it can be efficient to set splitOverConditions=true (see this tutorial).","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"Hessian method: For small models it is computationally feasible to compute an accurate full hessian via ForwardDiff.jl. For most models we benchmarked providing a hessian improved convergence. Thus, we recommend hessianMethod=:ForwardDiff.","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"Note1 - For models with preequilibration (steady-state simulations) our benchmarks suggest it might be better to use the Gauss-Newton hessian approximation.\nNote2 - For models where it too expansive to compute the full hessian (e.g. due to many simulation conditions) the hessian block approximation can be a good option.\nNote3 - In particular the interior-point Newton method from Optim.jl performs well if provided with a full hessian.","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"All-to-all, for a small model a good setup often is:","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"odeSolverOptions = getODESolverOptions(Rodas5P(), abstol=1e-8, reltol=1e-8)\npetabProblem = setupPEtabODEProblem(petabModel, odeSolverOptions, \n                                    gradientMethod=:ForwardDiff, \n                                    hessianMethod=:ForwardDiff)","category":"page"},{"location":"Best_options/#Medium-sized-models-(\\leq-75-parameters-and-\\leq-50-ODE:s)","page":"Choosing the best options for a PEtab problem","title":"Medium-sized models (leq 75 parameters and leq 50 ODE:s)","text":"","category":"section"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"ODE solver: For medium-sized stiff models the bdf-solvers such as QNDF() are among the fastest, while being sufficiently accurate. The drawback with Julia bdf-solvers is that for certain models (e.g. with many events) they frequently fail at low tolerances. If this happens a good plan b option is KenCarp4(). Another good option is Sundial's CVODE_BDF(), however it is not recommended since as it is written in C++ and thus is not compatible with forward-mode automatic differentiation. Regardless of solver we recommend to use low tolerances (around abstol, reltol = 1e-8, 1e-8) to obtain accurate gradients.","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"Gradient method: For medium-sized models when the hessian is approximated via the Gauss-Newton method (often performs best) we recommend computing the gradient via the forward sensitivities (gradientMethod=:ForwardEquations) where the sensitives are computed via forward-mode automatic differentiation (sensealg=:ForwardDiff). The main benefit is that if the optimizers always computes the gradient before the hessian these sensitivities can be reused when computing the hessian. Otherwise, if for example a BFGS hessian-approximation is used gradientMethod=:ForwardDiff often performs best.","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"Note1 - For :ForwardDiff the user can set the chunk-size. This can substantially improve performance, and we plan to add automatic tuning of it.\nNote2 - If the model has many simulation condition specific parameters (parameters that only appear in a subset of simulation conditions) it can be efficient to set splitOverConditions=true (see this tutorial).","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"Hessian method: For medium-sized models it is typically computationally infeasible to compute an accurate full hessian via ForwardDiff.jl. Rather we recommend the Gauss-Newton hessian approximation which often performs better than the commonly used (L)-BFGS approximation. Thus, we recommend hessianMethod=:GaussNewton.","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"Note1 - In particular trust-region Newton methods such as Fides.py performs well if provided with a full hessian. Interior-point methods do not perform as well.","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"All-to-all, in case the gradient is always computed before the optimizer hessian good setup often is:","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"odeSolverOptions = getODESolverOptions(QNDF(), abstol=1e-8, reltol=1e-8)\npetabProblem = setupPEtabODEProblem(petabModel, odeSolverOptions, \n                                    gradientMethod=:ForwardEquations, \n                                    hessianMethod=:GaussNewton, \n                                    reuseS=true)","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"Otherwise, a good setup is:","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"odeSolverOptions = getODESolverOptions(QNDF(), abstol=1e-8, reltol=1e-8)\npetabProblem = setupPEtabODEProblem(petabModel, odeSolverOptions, \n                                    gradientMethod=:ForwardDiff, \n                                    hessianMethod=:GaussNewton)","category":"page"},{"location":"Best_options/#Large-models-(\\geq-75-parameters-and-\\geq-50-ODE:s)","page":"Choosing the best options for a PEtab problem","title":"Large models (geq 75 parameters and geq 50 ODE:s)","text":"","category":"section"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"ODE solver: For large models we recommend too first benchmark different ODE solvers suitable for large models such as; QNDF(), FBDF(), KenCarp4(), and CVODE_BDF(). Moreover, we recommend too test providing the ODE solver with a sparse Jacobian (sparseJacobian::Bool=false), and to test different linear solvers such as CVODE_BDF(linsolve=:KLU). More details on how to efficiently solve a large stiff models can be found here.","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"Note - We strongly recommend comparing different ODE solvers as this can lead to a substantial reduction of runtime.","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"Gradient method: For large models the most scalable approach is adjoint sensitivity analysis (gradientMethod=:Adjoint). Currently, for sensealg we support InterpolatingAdjoint() and QuadratureAdjoint() from SciMLSensitivity (see their documentation for info), and as it is more reliable we recommend InterpolatingAdjoint().","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"Note1 - For adjoint sensitivity analysis we recommend to manually set the ODE solver gradient options. Currently, CVODE_BDF() outperform all native Julia solvers.\nNote2 - The user can provide any options InterpolatingAdjoint() and QuadratureAdjoint() accept.\nNote3 - Currently adjoint sensitivity analysis is not as reliable in Julia as in AMICI (see), but our benchmarks show that SciMLSensitivity has the potential to be faster.","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"Hessian method: For large models computing the sensitives (Gass-Newton) or a full hessian is not computationally feasible. Hence, the best option is often to use some L-(BFGS) approximation. BFGS support is built into most available optimizers such as Optim.jl, Ipopt.jl and Fides.py.","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"All-to-all for a large model a good setup often is:","category":"page"},{"location":"Best_options/","page":"Choosing the best options for a PEtab problem","title":"Choosing the best options for a PEtab problem","text":"odeSolverOptions = getODESolverOptions(CVODE_BDF(), abstol=1e-8, reltol=1e-8) \nodeSolverGradientOptions = getODESolverOptions(CVODE_BDF(), abstol=1e-8, reltol=1e-8) \npetabProblem = setupPEtabODEProblem(petabModel, odeSolverOptions, \n                                    odeSolverGradientOptions=odeSolverGradientOptions,\n                                    gradientMethod=:Adjoint, \n                                    sensealg=InterpolatingAdjoint()) ","category":"page"},{"location":"Brannmark/#Models-with-preequilibration-(steady-state-simulation)","page":"Models with preequilibration (steady-state simulation)","title":"Models with preequilibration (steady-state simulation)","text":"","category":"section"},{"location":"Brannmark/","page":"Models with preequilibration (steady-state simulation)","title":"Models with preequilibration (steady-state simulation)","text":"In this tutorial we show to create PEtabODEproblem for the small Branmark model which has a preequilibration condition. (≤ 75 states). This means that before the main simulation where we compare the model against data, the model must first be at a steady state du = f(u p t) approx 0 which can be achieved via","category":"page"},{"location":"Brannmark/","page":"Models with preequilibration (steady-state simulation)","title":"Models with preequilibration (steady-state simulation)","text":"Simulations\nRootfinding","category":"page"},{"location":"Brannmark/","page":"Models with preequilibration (steady-state simulation)","title":"Models with preequilibration (steady-state simulation)","text":"To run the code you need the Brannmark PEtab files which can be found here. A fully runnable example of this tutorial can be found here.","category":"page"},{"location":"Brannmark/","page":"Models with preequilibration (steady-state simulation)","title":"Models with preequilibration (steady-state simulation)","text":"First we read the model and load necessary libraries.","category":"page"},{"location":"Brannmark/","page":"Models with preequilibration (steady-state simulation)","title":"Models with preequilibration (steady-state simulation)","text":"using PEtab\nusing OrdinaryDiffEq\nusing Printf\n\npathYaml = joinpath(@__DIR__, \"Brannmark\", \"Brannmark_JBC2010.yaml\")\npetabModel = readPEtabModel(pathYaml, verbose=true)","category":"page"},{"location":"Brannmark/","page":"Models with preequilibration (steady-state simulation)","title":"Models with preequilibration (steady-state simulation)","text":"PEtabModel for model Brannmark. ODE-system has 9 states and 23 parameters.\nGenerated Julia files are at ...","category":"page"},{"location":"Brannmark/#Steady-state-solver","page":"Models with preequilibration (steady-state simulation)","title":"Steady-state solver","text":"","category":"section"},{"location":"Brannmark/","page":"Models with preequilibration (steady-state simulation)","title":"Models with preequilibration (steady-state simulation)","text":"For models with preequilibration before the main simulation we must solve for the steady state du = f(u p t)  0. This can be done via i) :Rootfinding where we use any algorithm from NonlinearSolve.jl to find the roots of f, and by ii) :Simulate where from the initial condition we simulate the model until it reaches a steady state. The latter is more stable and often performs best.","category":"page"},{"location":"Brannmark/","page":"Models with preequilibration (steady-state simulation)","title":"Models with preequilibration (steady-state simulation)","text":"When creating a PEtabODEProblem we can set steady-state solver options via the function getSteadyStateSolverOptions, where the first argument is the method to use; either :Rootfinding or :Simulate (recommended). For :Simulate we can choose how to terminate steady-state simulation via the howCheckSimulationReachedSteadyState argument which accepts:","category":"page"},{"location":"Brannmark/","page":"Models with preequilibration (steady-state simulation)","title":"Models with preequilibration (steady-state simulation)","text":":wrms : Weighted root-mean square sqrtsum_i=1^n bigg( fracduimathrmreltol*ui + mathrmabstol bigg)  frac1n leq 1 where n is the number of ODE:s.\n:Newton : If Newton-step Δu is sufficiently small sqrtsum_i=1^n bigg( fracDelta uimathrmreltol*ui + mathrmabstol bigg)  frac1n leq 1","category":"page"},{"location":"Brannmark/","page":"Models with preequilibration (steady-state simulation)","title":"Models with preequilibration (steady-state simulation)","text":"Newton often perform better but requires an invertible Jacobian. In case it is non-invertible the code switches automatically to :wrms. (abstol, reltol) defaults to ODE solver tolerances divided by 100.","category":"page"},{"location":"Brannmark/","page":"Models with preequilibration (steady-state simulation)","title":"Models with preequilibration (steady-state simulation)","text":"Below we use :Simulate with :wrms termination:","category":"page"},{"location":"Brannmark/","page":"Models with preequilibration (steady-state simulation)","title":"Models with preequilibration (steady-state simulation)","text":"odeSolverOptions = getODESolverOptions(Rodas5P(), abstol=1e-8, reltol=1e-8)\nssOptions = getSteadyStateSolverOptions(:Simulate,\n                                        howCheckSimulationReachedSteadyState=:wrms)\npetabProblem = setupPEtabODEProblem(petabModel, odeSolverOptions, \n                                    ssSolverOptions=ssOptions,\n                                    gradientMethod=:ForwardDiff) \np = petabProblem.θ_nominalT \ngradient = zeros(length(p)) \ncost = petabProblem.computeCost(p)\npetabProblem.computeGradient!(gradient, p)\n@printf(\"Cost= %.2f\\n\", cost)\n@printf(\"First element in the gradient = %.2e\\n\", gradient[1])","category":"page"},{"location":"Brannmark/","page":"Models with preequilibration (steady-state simulation)","title":"Models with preequilibration (steady-state simulation)","text":"Cost = 141.89\nFirst element in the gradient = 2.70e-03","category":"page"},{"location":"Brannmark/","page":"Models with preequilibration (steady-state simulation)","title":"Models with preequilibration (steady-state simulation)","text":"Some useful notes regarding the steady-state solver are:","category":"page"},{"location":"Brannmark/","page":"Models with preequilibration (steady-state simulation)","title":"Models with preequilibration (steady-state simulation)","text":"In case a SteadyStateSolverOption is not specified the default is :Simulate with :wrms.\nA separate steady-state solver option can also be set for the gradient via ssSolverGradientOptions.\nAll gradient and hessian options are compatible with :Simulate. :Rootfinding is only compatible with approaches using forward-mode automatic differentiation.","category":"page"},{"location":"Bachmann/#Medium-sized-models-(Bachmann-model)","page":"Medium sized models and adjoint sensitivity analysis","title":"Medium-sized models (Bachmann model)","text":"","category":"section"},{"location":"Bachmann/","page":"Medium sized models and adjoint sensitivity analysis","title":"Medium sized models and adjoint sensitivity analysis","text":"In this tutorial we show to create PEtabODEproblem for the medium-sized Bachmann model, and i) how to compute the gradient via forward-sensitivity equations, ii) compute the gradient via adjoint sensitivity analysis and iii) how to compute the Gauss-Newton hessian approximation. The latter often perform better than the (L)-BFGS Hessian approximation.","category":"page"},{"location":"Bachmann/","page":"Medium sized models and adjoint sensitivity analysis","title":"Medium sized models and adjoint sensitivity analysis","text":"To run the code you need the Bachmann PEtab files which can be found here. A fully runnable example of this tutorial can be found here.","category":"page"},{"location":"Bachmann/","page":"Medium sized models and adjoint sensitivity analysis","title":"Medium sized models and adjoint sensitivity analysis","text":"First we read the model and load necessary libraries.","category":"page"},{"location":"Bachmann/","page":"Medium sized models and adjoint sensitivity analysis","title":"Medium sized models and adjoint sensitivity analysis","text":"using PEtab\nusing OrdinaryDiffEq\nusing Sundials # For CVODE_BDF\nusing SciMLSensitivity # For adjoint\nusing Printf\n \npathYaml = joinpath(@__DIR__, \"Bachmann\", \"Bachmann_MSB2011.yaml\") \npetabModel = readPEtabModel(pathYaml, verbose=true)","category":"page"},{"location":"Bachmann/","page":"Medium sized models and adjoint sensitivity analysis","title":"Medium sized models and adjoint sensitivity analysis","text":"PEtabModel for model Bachmann. ODE-system has 25 states and 39 parameters.\nGenerated Julia files are at ...","category":"page"},{"location":"Bachmann/#Adjoint-sensitivity-analysis","page":"Medium sized models and adjoint sensitivity analysis","title":"Adjoint sensitivity analysis","text":"","category":"section"},{"location":"Bachmann/","page":"Medium sized models and adjoint sensitivity analysis","title":"Medium sized models and adjoint sensitivity analysis","text":"For a subset of medium-sized and definitely for big models gradients are most efficiently computed via adjoint sensitivity analysis (gradientMethod=:Adjoint). For adjoint there are several tuneable options that can improve performance, some key ones are:","category":"page"},{"location":"Bachmann/","page":"Medium sized models and adjoint sensitivity analysis","title":"Medium sized models and adjoint sensitivity analysis","text":"odeSolverGradientOptions - Which ODE solver and solver tolerances (abstol and reltol) to use when computing the gradient (in this case when solving the adjoint ODE-system). Below we use CVODE_BDF() which currently is the best performing stiff solver for the adjoint problem in Julia.\nsensealg - which adjoint algorithm to use. Currently, we support InterpolatingAdjoint and QuadratureAdjoint from SciMLSensitivity (see their documentation for info). The user can provide any of the options these methods are compatible with, so if you want to use the ReverseDiffVJP an acceptable option is; sensealg=InterpolatingAdjoint(autojacvec=ReversDiffVJP()).","category":"page"},{"location":"Bachmann/","page":"Medium sized models and adjoint sensitivity analysis","title":"Medium sized models and adjoint sensitivity analysis","text":"Note1 - currently adjoint sensitivity analysis is not as reliable in Julia as in AMICI (https://github.com/SciML/SciMLSensitivity.jl/issues/795), but our benchmarks show that SciMLSensitivity has the potential to be faster.\nNote2 - the compilation times can be quite substantial for adjoint sensitivity analysis.\nNote3 - below we use QNDF for the cost which often is one of the best Julia solvers for larger models.","category":"page"},{"location":"Bachmann/","page":"Medium sized models and adjoint sensitivity analysis","title":"Medium sized models and adjoint sensitivity analysis","text":"solverOptions = getODESolverOptions(QNDF(), abstol=1e-8, reltol=1e-8) \nsolverGradientOptions = getODESolverOptions(CVODE_BDF(), abstol=1e-8, reltol=1e-8) \npetabProblem = setupPEtabODEProblem(petabModel, solverOptions, \n                                    odeSolverGradientOptions=solverGradientOptions,\n                                    gradientMethod=:Adjoint, \n                                    sensealg=InterpolatingAdjoint(autojacvec=EnzymeVJP())) \np = petabProblem.θ_nominalT \ngradient = zeros(length(p)) \ncost = petabProblem.computeCost(p)\npetabProblem.computeGradient!(gradient, p)\n@printf(\"Cost = %.2f\\n\", cost)\n@printf(\"First element in the gradient = %.2e\\n\", gradient[1])","category":"page"},{"location":"Bachmann/","page":"Medium sized models and adjoint sensitivity analysis","title":"Medium sized models and adjoint sensitivity analysis","text":"Cost = -418.41\nFirst element in the gradient = -1.70e-03","category":"page"},{"location":"Bachmann/#Forward-sensitivity-analysis-and-Gauss-Newton-hessian-approximation","page":"Medium sized models and adjoint sensitivity analysis","title":"Forward sensitivity analysis and Gauss-Newton hessian approximation","text":"","category":"section"},{"location":"Bachmann/","page":"Medium sized models and adjoint sensitivity analysis","title":"Medium sized models and adjoint sensitivity analysis","text":"For medium-sized models computing the full Hessian via forward-mode automatic differentiation is often too expansive, thus we need some approximation. The Guass-Newton (GN)approximation often performs better than the (L)-BFGS approximation. To compute it we need the forward sensitivities. These sensitives can also be used to compute the gradient. As some optmizers such as Fides.py compute both the hessian and gradient at each iteration we can be smart here and save the sensitives between the gradient and hessian computations.","category":"page"},{"location":"Bachmann/","page":"Medium sized models and adjoint sensitivity analysis","title":"Medium sized models and adjoint sensitivity analysis","text":"When choosing gradientMethod=:ForwardEquations and hessianMethod=:GaussNewton there are several tuneable options, key ones are:","category":"page"},{"location":"Bachmann/","page":"Medium sized models and adjoint sensitivity analysis","title":"Medium sized models and adjoint sensitivity analysis","text":"sensealg - which sensitivity algorithm to use when computing for the sensitives. We support both ForwardSensitivity() and ForwardDiffSensitivity() with tuneable options as provided by SciMLSensitivity (see their documentation for info). The most efficient option though is :ForwardDiff where forward mode automatic differentiation is used to compute the sensitivities.\nreuseS::Bool - whether to reuse the sensitives from the gradient computations when computing the Gauss-Newton hessian-approximation. Whether this option is applicable depends on the optimizers, for example it works with Fides.py but not Optim.jl:s IPNewton().\nNote - this approach requires that sensealg=:ForwardDiff for the gradient.","category":"page"},{"location":"Bachmann/","page":"Medium sized models and adjoint sensitivity analysis","title":"Medium sized models and adjoint sensitivity analysis","text":"odeSolverOptions = getODESolverOptions(QNDF(), abstol=1e-8, reltol=1e-8) \npetabProblem = setupPEtabODEProblem(petabModel, odeSolverOptions, \n                                    gradientMethod=:ForwardEquations, \n                                    hessianMethod=:GaussNewton,\n                                    sensealg=:ForwardDiff, \n                                    reuseS=true) \np = petabProblem.θ_nominalT \ngradient = zeros(length(p)) \nhessian = zeros(length(p), length(p)) \ncost = petabProblem.computeCost(p)\npetabProblem.computeGradient!(gradient, p)\npetabProblem.computeHessian!(hessian, p)\n@printf(\"Cost for Bachmann = %.2f\\n\", cost)\n@printf(\"First element in the gradient = %.2e\\n\", gradient[1])\n@printf(\"First element in the Gauss-Newton Hessian = %.2f\\n\", hessian[1, 1])","category":"page"},{"location":"Bachmann/","page":"Medium sized models and adjoint sensitivity analysis","title":"Medium sized models and adjoint sensitivity analysis","text":"Cost for Bachmann = -418.41\nFirst element in the gradient = -1.85e-03\nFirst element in the Gauss-Newton Hessian = 584.10","category":"page"},{"location":"Beer/#Beer_tut","page":"Models with many conditions specific parameters","title":"Models with many conditions specific parameters","text":"","category":"section"},{"location":"Beer/","page":"Models with many conditions specific parameters","title":"Models with many conditions specific parameters","text":"In this tutorial we show to create PEtabODEproblem for a small ODE-model (≤20 states, ≤20 parameters) with many parameters to estimate (≈70), because most parameter are specific to a subset of simulation conditions. For example, in cond1 we have τcond1 and in cond2 we have τcond2 that maps to the ODE-system parameter τ respectively.","category":"page"},{"location":"Beer/","page":"Models with many conditions specific parameters","title":"Models with many conditions specific parameters","text":"To run the code you need the Beer PEtab files which can be found here. A fully runnable example of this tutorial can be found here.","category":"page"},{"location":"Beer/","page":"Models with many conditions specific parameters","title":"Models with many conditions specific parameters","text":"First we read the model and load necessary libraries.","category":"page"},{"location":"Beer/","page":"Models with many conditions specific parameters","title":"Models with many conditions specific parameters","text":"using PEtab\nusing OrdinaryDiffEq\nusing Printf\n\npathYaml = joinpath(@__DIR__, \"Beer\", \"Beer_MolBioSystems2014.yaml\") \npetabModel = readPEtabModel(pathYaml, verbose=true)","category":"page"},{"location":"Beer/","page":"Models with many conditions specific parameters","title":"Models with many conditions specific parameters","text":"PEtabModel for model Beer. ODE-system has 4 states and 9 parameters.\nGenerated Julia files are at ...","category":"page"},{"location":"Beer/#Handling-condition-specific-parameters","page":"Models with many conditions specific parameters","title":"Handling condition specific parameters","text":"","category":"section"},{"location":"Beer/","page":"Models with many conditions specific parameters","title":"Models with many conditions specific parameters","text":"For a small ODE-system like Beer the most efficient gradient method is gradientMethod=:ForwardDiff, and we can further compute the hessian via hessianMethod=:ForwardDiff. However, there are several condition specific parameters, and as we by default we compute the gradient and hessian via a single call to ForwardDiff.jl it means that for this model we have to perform as many forward-passes (solve the ODE model) as there are model-parameters. To force one ForwardDiff.jl call per simulation condition we can use the option splitOverConditions=true. Thus, for a model like Beer good options are:","category":"page"},{"location":"Beer/","page":"Models with many conditions specific parameters","title":"Models with many conditions specific parameters","text":"odeSolverOptions - Rodas5P() (works well for smaller models ≤ 15 states), and we use the default abstol, reltol .= 1e-8.\ngradientMethod - For small models like Beer forward mode automatic differentiation (AD) is fastest, thus below we choose :ForwardDiff.\nhessianMethod - For small models like Boehm with ≤20 parameters it is computationally feasible to compute the full Hessian via forward-mode AD. Thus, we choose :ForwardDiff.\nsplitOverConditions=true - Force a call to ForwardDiff.jl per simulation condition. Most efficient for models where a majority of parameters are specific to a subset of simulation conditions.","category":"page"},{"location":"Beer/","page":"Models with many conditions specific parameters","title":"Models with many conditions specific parameters","text":"odeSolverOptions = getODESolverOptions(Rodas5P(), abstol=1e-8, reltol=1e-8)\npetabProblem = setupPEtabODEProblem(petabModel, odeSolverOptions, \n                                    gradientMethod=:ForwardDiff, \n                                    hessianMethod=:ForwardDiff, \n                                    splitOverConditions=true)\n\np = petabProblem.θ_nominalT\ngradient = zeros(length(p))\nhessian = zeros(length(p), length(p))\ncost = petabProblem.computeCost(p)\npetabProblem.computeGradient!(gradient, p)\npetabProblem.computeHessian!(hessian, p)\n@printf(\"Cost = %.2f\\n\", cost)\n@printf(\"First element in the gradient = %.2e\\n\", gradient[1])\n@printf(\"First element in the hessian = %.2f\\n\", hessian[1, 1])","category":"page"},{"location":"Beer/","page":"Models with many conditions specific parameters","title":"Models with many conditions specific parameters","text":"Cost = -58622.91\nFirst element in the gradient = 7.17e-02\nFirst element in the hessian = 755266.33","category":"page"},{"location":"#PEtab.jl","page":"Home","title":"PEtab.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is the documentation of PEtab.jl, a Julia package that imports ODE parameter estimation problem specified in the PEtab format into Julia.","category":"page"},{"location":"","page":"Home","title":"Home","text":"By leveraging the ODE solvers in Julia’s DifferentialEquations.jl package, and symbolic model processing via ModelingToolkit.jl, PEtab.jl achieves fast model simulations. This combined with support for gradients via forward- and adjoint-sensitivity approaches, and hessian via both exact and approximate methods means PEtab.jl provides everything needed to perform efficient parameter estimation for both big and small models. In an extensive benchmark study (soon to appear on Archive) PEtab.jl was in many cases 3-4 faster than the PyPesto toolbox which leverages the AMICI interface to the Sundial's suite.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In this documentation you can find:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Getting started (a recommended introduction)\nTutorials for medium-sized models, small models with several conditions specific parameters, and models with pre-equilibration conditions (steady-state simulations).\nAvailable hessian and gradient options.\nA discussion on best options for specific model types (small, medium and large models)","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To read SBML files PEtab.jl uses the Python python-libsbml library which can be installed via:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pip install python-libsbml","category":"page"},{"location":"","page":"Home","title":"Home","text":"Given this PEtab.jl can be installed via","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ] add PEtab","category":"page"},{"location":"#Feature-list","page":"Home","title":"Feature list","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Symbolic model preprocessing via ModelingToolkit\nSupport for all ODE solvers in DifferentialEquations.jl\nGradient via:\nForward-mode automatic differentiation using ForwardDiff.jl\nForward sensitivity analysis either via ForwardDiff.jl or SciMLSensitivity.jl\nAdjoint sensitivity via any of the algorithms in SciMLSensitivity.jl\nAutomatic differentiation via Zygote.jl\nHessians computed\n“exactly\" via forward-mode automatic differentiation using ForwardDiff.jl\napproximately via a block approach using ForwardDiff.jl\napproximately via the Gauss-Newton method (often outperform (L)-BFGS)\nPre-equilibration and pre-simulation conditions\nSupport for models with discrete events and logical operations","category":"page"},{"location":"#Citation","page":"Home","title":"Citation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We will soon publish a preprint you can cite if you found PEtab.jl helpful in your work.","category":"page"},{"location":"API_choosen/#API","page":"API","title":"API","text":"","category":"section"},{"location":"API_choosen/","page":"API","title":"API","text":"PEtabModel\nPEtabODEProblem\nODESolverOptions\nSteadyStateSolverOptions\nreadPEtabModel\nsetupPEtabODEProblem\ngetODESolverOptions\ngetSteadyStateSolverOptions","category":"page"},{"location":"API_choosen/#PEtab.PEtabModel","page":"API","title":"PEtab.PEtabModel","text":"PEtabModel\n\nA PEtab specified problem translated into a Julia compatible format.\n\nCreated from `readPEtabModel` contains helper functions needed to set up cost-, gradient-, hessian-computations, and \nfor handling potential model events (callbacks). \n\nNote1 - Several of the functions in the PEtabModel are not meant to be accessible for the user. For example \ncompute_h (and similar functions) require indices which are built in the background to efficiently map parameter \nbetween experimental (simulation) conditions. Rather, `PEtabModel` holds all information needed to create a \nPEtabODEProblem, and in the future PEtabSDEProblem etc ...\nNote2 - ODEProblem.p refers to the parameters for underlying DifferentialEquations.jl ODEProblem.\n\n# Fields\n`modelName`: Model-name extracted from the PEtab yaml-file. \n`compute_h`: Compute the observable (h) for a specific time-point and simulation condition.\n`compute_u0!`: In-place initial values using the ODEProblem.p for a simulation condition; compute_u0!(u0, p)\n`compute_u0`: As above but not in-place; u0 = compute_u0(p)\n`compute_σ`: Compute the noise parameter σ for specific time-point and simulation condition.\n`compute_∂h∂u!`: Compute the gradient of h with respect to ODE-model states (u) for a specific time-point and \n simulation condition.\n`compute_∂σ∂u!`: As above but for the noise parameter σ\n`compute_∂h∂p!`: As above for h but with respect to ODEProblem.p\n`compute_∂σ∂p!`: As above for σ but with respect to ODEProblem.p\n`computeTStops`: In case the model has DiscreteCallbacks (events) this function computes the event times. \n`convertTspan::Bool`: In case the model has DiscreteCallbacks (events) and the trigger-time is a parameter set to \n be estimated this Bool tracks that for ForwardDiff.jl gradients the time-span should be converted to Dual-numbers. \n`dirModel`: Directory where the model.xml and PEtab files are stored.\n`dirJulia`: Directory where the Julia-model files created by parsing the PEtab files (e.g SBML-file) are stored. \n`odeSystem`: A ModellingToolkit.jl ODE-system obtained from parsing the model SBML-file.  \n`parameterMap`: A ModellingToolkit.jl parameter map for the ODE-system.\n`stateMap`: A ModellingToolkit.jl state map for the ODE-system describing how the inital values are computed, e.g.\n whether or not certain initial values are computed from parameters in the parameterMap.\n`parameterNames`: Names of the parameter in the odeSystem.\n`stateNames`: Names of the states in the odeSystem.\n`pathMeasurements`: Path to the PEtab measurements file\n`pathConditions`: Path to the PEtab conditions file\n`pathObservables`: Path to the PEtab observables file\n`pathParameters`: Path to the PEtab parameters file\n`pathSBML`: Path to the PEtab SBML file\n`pathYAML`: Path to the PEtab yaml file\n`modelCallbackSet`: Stores potential model callbacks (events)\n`checkIfCallbackIsActive`: Piecewise SBML statements are rewritten to DiscreteCallbacks that are activated at a\nspecific time-point. The piecewise callback has a defult value at t0 which is only triggered upon reaching t_activation.\nIn case t_activation ≤ 0 (never reached when solvig the model) this function checks whether or not the callback \nshould be triggered before solving the model.\n\n\n\n\n\n","category":"type"},{"location":"API_choosen/#PEtab.PEtabODEProblem","page":"API","title":"PEtab.PEtabODEProblem","text":"PEtabODEProblem\n\nAll needed to setup an optimization problem (compute cost, gradient, hessian and parameter bounds) for a PEtab model.\n\nThe PEtabODEproblem for a PEtab problem allows for efficient cost, gradient and hessian computations. Constructed\nvia `setupPEtabODEProblem`, more info on tuneable options can be found in the documentation [add]. \n\n**Note** - the parameter vector θ is **always** assumed to be on parameter scale specified in the PEtab parameters \nfile. If needed θ is transformed to linear scale inside of the function call. \n\n# Fields\n`computeCost`: For θ computes the objective value cost = computeCost(θ)\n`computeGradient!`: For θ computes in-place gradient computeGradient!(gradient, θ)\n`computeHessian!`: For θ computes in-place hessian-(approximation) computeHessian!(hessian, θ)\n`costMethod`: Method for computing the cost (:Standard, :Zygote)\n`gradientMethod`: Method for computing the gradient (:ForwardDiff, :ForwardEquations :Adjoint, :Zygote)\n`hessianMethod`:  Method for computing/approximating the hessian (:ForwardDiff, :BlocForwardDiff :GaussNewton)\n`nParametersToEstimate`: Number of parameter to estimate.\n`θ_estNames`: Names of the parameter in θ\n`θ_nominal`: Nominal θ values as specified in the PEtab parameters-file. \n`θ_nominalT`: Nominal θ values on parameter-scale (e.g log) as specified in the PEtab parameters-file.\n`lowerBounds`: Lower parameter bounds on parameter-scale for θ as specified in the PEtab parameters-file.\n`upperBounds`: Upper parameter bounds on parameter-scale for θ as specified in the PEtab parameters-file.\n`petabModel`: PEtabModel used to construct the PEtabODEProblem\n`odeSolverOptions`: ODE-solver options specified when creating the PEtabODEProblem \n`odeSolverGradientOptions`: ODE-solver gradient options specified when creating the PEtabODEProblem\n\n\n\n\n\n","category":"type"},{"location":"API_choosen/#PEtab.ODESolverOptions","page":"API","title":"PEtab.ODESolverOptions","text":"ODESolverOptions\n\nStores ODE-solver options (solver, tolerances, etc...) to use when computing gradient/cost for a PEtabODEProblem. \n\nConstructed via `getODESolverOptions`. More info regarding the options and available solvers can be found in the \ndocumentation for DifferentialEquations.jl (https://docs.sciml.ai/DiffEqDocs/stable/solvers/ode_solve/), and \nin the documentation for `getODESolverOptions`.\n\n# Fields\n`solver`: Any of the ODE-solvers in DifferentialEquations.jl\n`abstol`: Absolute tolerance when solving the ODE-system. \n`reltol`: Relative tolerance when solving the ODE-system\n`force_dtmin`: Whether or not to force dtmin when solving the ODE-system.\n`dtmin`: Minimal acceptable step-size when solving the ODE-system.\n`maxiters`: Maximum number of iterations when solving the ODE-system.\n\n\n\n\n\n","category":"type"},{"location":"API_choosen/#PEtab.SteadyStateSolverOptions","page":"API","title":"PEtab.SteadyStateSolverOptions","text":"SteadyStateSolverOptions\n\nStores options (algorithm, tolerances, etc...) to use when computing steady state for models with pre-equlibration.\n\nConstructed via `getSteadyStateSolverOptions` with several potential user options.\n\n# Fields\n`method`: Approach to find steady-state u*; du = f(u*, p, t) ≈ 0. Either :Rootfinding to directly solve the problem \n via optimisation, or :Simulate to via ODE solver simulate model to steady state.\n`rootfindingAlgorithm`: In case of :Rootfinding which algorithm to use. Supports any of the NonlinearSolve algorithms \n (https://docs.sciml.ai/NonlinearSolve/stable/tutorials/nonlinear/).\n`howCheckSimulationReachedSteadyState`: For :Simulate which method to check steady state been reached, options;\n    wrms : Weighted root-mean square : √(∑((du ./ (reltol * u .+ abstol)).^2) / length(u)) < 1\n    Newton : If Newton-step Δu is sufficiently small : √(∑((Δu ./ (reltol * u .+ abstol)).^2) / length(u)) < 1\n`abstol`: Absolute tolerance when checking if steady state has been found. Defaults to 1e-8 for :Rootfinding and\n ODE-solver tolerance divided by 100 for :Simulate\n`reltol`: Relative tolerance when checking if steady state has been found. As for abstol.\n`maxiters`: Maximum number of root-finding or ODE-solver steps when solving for steady state. Defaults to 1e4\n for :Rootfinding and ODE-solver options for :Simulate.\n\n\n\n\n\n","category":"type"},{"location":"API_choosen/#PEtab.readPEtabModel","page":"API","title":"PEtab.readPEtabModel","text":"readPEtabModel(pathYAML::String;\n               forceBuildJuliaFiles::Bool=false,\n               verbose::Bool=true,\n               ifElseToEvent::Bool=true)::PEtabModel\n\nParses a PEtab specified problem with yaml-file at `pathYAML` into a Julia accessible format. \n\nWhen parsing a PEtab problem several things happens under the hood;\n1) The SBML file is translated into ModelingToolkit.jl format (e.g allow symbolic computations of the ODE-model \n   Jacobian). Piecewise and model events are further written into DifferentialEquations.jl callbacks.\n2) The observable PEtab-table is translated into Julia-file with functions for computing the observable (h), \n   noise parameter (σ) and initial values (u0). \n3) To allow gradients via adjoint sensitivity analysis and/or forward sensitivity equations the gradients of \n   h and σ are computed symbolically with respect to the ODE-models states (u) and parameters (odeProblem.p).\nAll this happens automatically, and resulting files are stored under petabModel.dirJulia. To save time \n`forceBuildJlFiles=false` meaning that Julia files are not rebuilt in case the already exist.\n\nIn the future we plan to allow the user to also provide a Julia file instead of a SBML file.\n\n\n\n\n\n","category":"function"},{"location":"API_choosen/#PEtab.setupPEtabODEProblem","page":"API","title":"PEtab.setupPEtabODEProblem","text":"setupPEtabODEProblem(petabModel::PEtabModel, \n                     odeSolverOptions::ODESolverOptions; \n                     <keyword arguments>)\n\nFor a PEtabModel and ODE-solver options (e.g. solver and tolerances) returns a PEtabODEProblem.\n\nThe PEtabODEproblem allows for efficient cost, gradient and hessian computations for a PEtab specified problem.  \nUsing the keyword arguments (see below) the user can select cost method, gradient method, hessian method, ODE \nsolver options, and a few tuneable options that potentially can make computations more efficient for a subset of \n\"edge-case\" models. A discussion about the most efficient option for different model types can be found in the \ndocumentation [add]. \n\n# Arguments\n- `petabModel::PEtabModel`: a PEtab-specified problem processed into Julia syntax by `readPEtabModel`\n- `odeSolverOptions::ODESolverOptions`: ODE-solver options when computing the cost (e.g solver and tolerances)\n- `odeSolverGradientOptions=nothing` : ODE-solver options when computing the gradient, e.g. the ODE solver options \n   used when doing adjoint sensitivity analysis. If nothing defaults to `odeSolverOptions`. \n- `ssSolverOptions=nothing` : Options used when solving for steady-state for models with pre-equlibrium. Steady-state\n   can be found either via simulation or rootfinding and can be set via `getSteadyStateSolverOptions` (see \n   documentation), if nothing defaults to simulation with wrms < 1 termination.\n   used when doing adjoint sensitivity analysis. If nothing defaults to `odeSolverOptions`. \n- `ssSolverGradientOptions=nothing` : Options used when solving for steady-state for models with pre-equlibrium when\n   doing gradient computations. If nothing defaults to `ssSolverOptions` value.\n- `costMethod::Symbol=:Standard` : method for computing the cost (objective). Two options are available, :Standard is \n   most efficient, while :Zygote is less efficient but compatible with the Zygote automatic differentiation library.\n- `gradientMethod::Symbol=:ForwardDiff` : method for computing the gradient of the (objective). Four availble options:\n    * :ForwardDiff - Compute the gradient via forward-mode automatic differentiation using ForwardDiff.jl. Most \n      efficient for models with ≤50 parameters. Optionally the number of chunks can be set by `chunkSize`.\n    * :ForwardEquations - Compute the gradient via the model sensitivities, where `sensealg` species how to solve \n      for the sensitivities. Most efficient if the hessian is approximated via the Gauss-Newton method, and if in the \n      optimizer we can reuse the sensitives (see `reuseS`) from the gradient computations in the hessian computations \n      (e.g when the optimizer always computes the gradient before the hessian). \n    * :Adjoint - Compute the gradient via adjoint sensitivity analysis, where `sensealg` specifies which algorithm \n      to use. Most efficient for large models (≥75 parameters). \n    * :Zygote - Compute the gradient via the Zygote package, where `sensealg` specifies which sensitivity algorithm \n      to use when solving the ODE-model. Most inefficient option and not recommended to use at all. \n- `hessianMethod::Symbol=:ForwardDiff` : method for computing the hessian of the cost. Three available options:\n    * :ForwardDiff - Compute the hessian via forward-mode automatic differentiation using ForwardDiff.jl. Often only \n      computationally feasible for models with ≤20 parameters, but often greatly improves optimizer convergence. \n    * :BlockForwardDiff - Compute hessian block approximation via forward-mode automatic differentiation using \n      ForwardDiff.jl. Approximation consists of two block matrices, the first is the hessian for only the dynamic \n      parameters (parameter part of the ODE system), and the second for the non-dynamic parameters (e.g noise \n      parameters). Computationally feasible for models with ≤ 20 dynamic parameters and often performs better than \n      BFGS-methods. \n    * :GaussNewton - Approximate the hessian via the Gauss-Newton method. Often performs better than the BFGS method.\n      If in the optimizer we can reuse the sensitives from the gradient (see `reuseS`) this method is best paired with \n      `gradientMethod=:ForwardEquations`. \n- `sparseJacobian::Bool=false` : when solving the ODE du/dt=f(u, p, t) whether or not for implicit solvers use a \n   sparse-jacobian. Sparse jacobian often performs best for large models (≥100 states). \n- `specializeLevel=SciMLBase.FullSpecialize` : specialization level when building the ODE-problem. Not recommended \n   to change (see https://docs.sciml.ai/SciMLBase/stable/interfaces/Problems/)\n- `sensealg=InterpolatingAdjoint()` : Sensitivity algorithm for gradient computations. Available options for each \n   gradient method are:\n    * :ForwardDiff : None (as ForwardDiff takes care of all computation steps)\n    * :ForwardEquations : :ForwardDiff (uses ForwardDiff.jl) or ForwardDiffSensitivity() and ForwardSensitivity() \n      from SciMLSensitivity.jl (https://github.com/SciML/SciMLSensitivity.jl). \n    * :Adjoint : InterpolatingAdjoint() and QuadratureAdjoint() from SciMLSensitivity.jl\n    * :Zygote : all sensealg in SciMLSensitivity.jl \n- `sensealgSS=InterpolatingAdjoint()` : Sensitivity algorithm for adjoint gradient compuations for steady state \n   simulations. Availble options are SteadyStateAdjoint() InterpolatingAdjoint() and QuadratureAdjoint() from \n   SciMLSensitivity.jl. SteadyStateAdjoint() is most efficient but requires a non-singular jacobian, and in case\n   of non-singular jacobian the code automatically switches to InterpolatingAdjoint(). \n- `chunkSize=nothing` : Chunk-size for ForwardDiff.jl when computing the gradient and hessian via forward mode \n   automatic different. If nothing default value is used. Tuning chunkSize is non-trivial and we plan to add \n   automatic functionality for this.\n- `splitOverConditions::Bool=false` : For gradient and hessian via ForwardDiff.jl whether or not to split calls to \n  to ForwardDiff across experimental (simulation) conditions. Should only be set to true in case the model has many \n  parameters tgat are specific to an experimental condition, else the overhead from the calls will increase run time. \n  See the Beer-example for an example where this is needed.\n- `reuseS::Bool=false` : Reuse the sensitives from the gradient computations for the Gauss-Newton hessian approximation.\n  Only applicable when `hessianMethod=:GaussNewton` and `gradientMethod=:ForwardEquations` and should **only** be used \n  when the optimizer **always** computes the gradient before the hessian.\n- `verbose::Bool=true` : Print progress when setting up PEtab ODEProblem\n\n\n\n\n\n","category":"function"},{"location":"API_choosen/#PEtab.getODESolverOptions","page":"API","title":"PEtab.getODESolverOptions","text":"getODESolverOptions(solver, <keyword arguments>)\n\nSetup ODE-solver options (solver, tolerances, etc...) to use when computing gradient/cost for a PEtabODEProblem. \n\nMore info of about the options and available solvers can be found in the documentation for DifferentialEquations.jl \n(https://docs.sciml.ai/DiffEqDocs/stable/solvers/ode_solve/). Recommendeded settings for which solver and options \nto use for different problems can be found below and in the documentation.\n\n# Arguments\n`solver`: Any of the ODE-solvers in DifferentialEquations.jl. For small (≤20 states) mildly stiff models \n composite solvers such as `AutoVern7(Rodas5P())` perform well. For stiff small models `Rodas5P()` performs \n well. For medium sized models (≤75states) `QNDF()`, `FBDF()` and `CVODE_BDF()` perform well. `CVODE_BDF()` is \n not compatible with automatic differentiation and thus cannot be used if the gradient is computed via automatic \n differentiation, or if the Gauss-Newton hessian approximation is used. If the gradient is computed via adjoint \n sensitivity analysis `CVODE_BDF()` is often the best choices as it typically is more relaible than `QNDF()` and \n `FBDF()` (fails less often).\n`abstol=1e-8`: Absolute tolerance when solving the ODE-system. Not recommended to increase above 1e-6 for gradients. \n`reltol=1e-8`: Relative tolerance when solving the ODE-system. Not recommended to increase above 1e-6 for gradients. \n`force_dtmin=false`: Whether or not to force dtmin when solving the ODE-system.\n`dtmin=nothing`: Minimal acceptable step-size when solving the ODE-system.\n`maxiters=10000`: Maximum number of iterations when solving the ODE-system. Increasing above the default value can \n cause the optimization to take substantial time.\n\n\n\n\n\n","category":"function"},{"location":"API_choosen/#PEtab.getSteadyStateSolverOptions","page":"API","title":"PEtab.getSteadyStateSolverOptions","text":"getSteadyStateSolverOptions(method::Symbol;\n                            howCheckSimulationReachedSteadyState::Symbol=:wrms,\n                            rootfindingAlgorithm=nothing,\n                            abstol=nothing, \n                            reltol=nothing, \n                            maxiters=nothing)::SteadyStateSolverOptions\n\nSetup steady-state solver options for finding steady-state via **either** method=:Rootfinding or method=:Simulate.\n\nFor :Rootfinding the steady state u* is found by solving the problem du = f(u, p, t) ≈ 0 with tolerances \nabstol and reltol via an automatically choosen optimisation algorithm (rootfindingAlgorithm=nothing) or via any \nalgorithm in NonlinearSolve.jl (https://docs.sciml.ai/NonlinearSolve/stable/solvers/NonlinearSystemSolvers/), e.g. \nrootfindingAlgorithm=NonlinearSolve.TrustRegion(). (abstol, reltol, maxiters) defaults to (1e-8, 1e-8, 1e4).\n\nFor :Simulate the steady state u* is found by simulating the ODE-system until du = f(u, p, t) ≈ 0.\nTwo options are availble for howCheckSimulationReachedSteadyState;\n    - :wrms : Weighted root-mean square √(∑((du ./ (reltol * u .+ abstol)).^2) / length(u)) < 1\n    - :Newton : If Newton-step Δu is sufficiently small √(∑((Δu ./ (reltol * u .+ abstol)).^2) / length(u)) < 1. \nNewton often perform better but requires an invertible Jacobian. In case not fulfilled code switches automatically\nto wrms. (abstol, reltol) defaults to ODE solver tolerances divided by 100 and maxiters to ODE solver value.\n    \nmaxiters refers to either maximum number of rootfinding steps, or maximum number of integration steps.\n\n\n\n\n\n","category":"function"},{"location":"Gradient_hessian_support/#gradient_support","page":"Supported gradient and hessian methods","title":"Supported gradient and hessian methods","text":"","category":"section"},{"location":"Gradient_hessian_support/","page":"Supported gradient and hessian methods","title":"Supported gradient and hessian methods","text":"PEtab.jl supports several gradient and hessian methods when building a PEtabODEProblem via setupPEtabODEProblem. Here we briefly cover each method and its associated tuneable parameters.","category":"page"},{"location":"Gradient_hessian_support/#Gradient-methods","page":"Supported gradient and hessian methods","title":"Gradient methods","text":"","category":"section"},{"location":"Gradient_hessian_support/","page":"Supported gradient and hessian methods","title":"Supported gradient and hessian methods","text":":ForwardDiff: Compute the gradient via forward mode automatic differentiation using ForwardDiff. Via the argument chunkSize the user can set the Chunk-size (see here). This can improve performance, and we plan to add automatic tuning of it.\n:ForwardEquations: Compute the gradient via the forward sensitivities. Via the sensealg argument the user can choose method for computing sensitivities. We support both ForwardSensitivity() and ForwardDiffSensitivity() with tuneable options as provided by SciMLSensitivity (see their documentation for info). The most efficient option though is sensealg=:ForwardDiff where forward mode automatic differentiation is used to compute the sensitivities.\n:Adjoint: Compute the gradient via adjoint sensitivity analysis. Via the sensealg argument the user can choose between the methods InterpolatingAdjoint and QuadratureAdjoint from SciMLSensitivity (see their documentation for info). The user can provide any of the options that these methods accept.\n:Zygote: Compute the gradient using the Zygote automatic differentiation library. Via the sensealg the user can choose any of the methods provided by SciMLSensitivity.\nNote: As the code relies heavily on for-loops :Zygote is by far the slowest option and is not recommended.","category":"page"},{"location":"Gradient_hessian_support/#Hessian-methods","page":"Supported gradient and hessian methods","title":"Hessian methods","text":"","category":"section"},{"location":"Gradient_hessian_support/","page":"Supported gradient and hessian methods","title":"Supported gradient and hessian methods","text":":ForwardDiff: Compute the hessian via forward mode automatic differentiation using ForwardDiff.\n:BlockForwardDiff: Compute a Hessian block approximation via forward mode automatic differentiation using ForwardDiff. In general for a PEtab model we have two set of parameters to estimate, parameters part of the ODE-system theta_p and parameter which are not a part of the ODE-system theta_q. This approach computes the hessian for each block and assumes zero-valued cross-terms:","category":"page"},{"location":"Gradient_hessian_support/","page":"Supported gradient and hessian methods","title":"Supported gradient and hessian methods","text":"    H_block = \n    beginbmatrix\n    H_p  mathbf0 \n    mathbf0  mathbfH_q\n    endbmatrix","category":"page"},{"location":"Gradient_hessian_support/","page":"Supported gradient and hessian methods","title":"Supported gradient and hessian methods","text":"This approach often works well if the number of non-dynamic parameters in theta_q are few.","category":"page"},{"location":"Gradient_hessian_support/","page":"Supported gradient and hessian methods","title":"Supported gradient and hessian methods","text":":GaussNewton: Compute a Hessian approximation via the Gauss-Newton method. This method often performs better often performs better than a (L)-BFGS approximation. However, it requires access sensitives which are only feasible to compute for smaller models (leq 75 parameters).","category":"page"}]
}
