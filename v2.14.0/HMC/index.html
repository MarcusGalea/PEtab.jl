<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Bayesian inference · PEtab.jl</title><meta name="title" content="Bayesian inference · PEtab.jl"/><meta property="og:title" content="Bayesian inference · PEtab.jl"/><meta property="twitter:title" content="Bayesian inference · PEtab.jl"/><meta name="description" content="Documentation for PEtab.jl."/><meta property="og:description" content="Documentation for PEtab.jl."/><meta property="twitter:description" content="Documentation for PEtab.jl."/><meta property="og:url" content="https://sebapersson.github.io/PEtab.jl/HMC/"/><meta property="twitter:url" content="https://sebapersson.github.io/PEtab.jl/HMC/"/><link rel="canonical" href="https://sebapersson.github.io/PEtab.jl/HMC/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">PEtab.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../Boehm/">Importing problems in PEtab standard format</a></li><li><span class="tocitem">Defining a PEtab problem in Julia</span><ul><li><a class="tocitem" href="../Define_in_julia/">Defining parameter estimation problems in Julia</a></li><li><a class="tocitem" href="../Julia_steady_state/">Pre-equilibration (steady-state simulations)</a></li><li><a class="tocitem" href="../Julia_obs_noise/">Noise and observable parameters</a></li><li><a class="tocitem" href="../Julia_condition_specific/">Condition specific system/model parameters</a></li><li><a class="tocitem" href="../Julia_event/">Events (callbacks, dosages etc...)</a></li></ul></li><li><span class="tocitem">Options for specific problem types</span><ul><li><a class="tocitem" href="../Brannmark/">Models with pre-equilibration (steady-state simulation)</a></li><li><a class="tocitem" href="../Bachmann/">Medium sized models and adjoint sensitivity analysis</a></li><li><a class="tocitem" href="../Beer/">Models with many conditions specific parameters</a></li></ul></li><li><span class="tocitem">Parameter estimation</span><ul><li><a class="tocitem" href="../Parameter_estimation/">Parameter estimation</a></li><li><a class="tocitem" href="../Avaible_optimisers/">Available optimisers</a></li><li><a class="tocitem" href="../Model_selection/">Model selection (PEtab select)</a></li><li><a class="tocitem" href="../optimisation_output_plotting/">Plots evaluating parameter estimation</a></li></ul></li><li class="is-active"><a class="tocitem" href>Bayesian inference</a><ul class="internal"><li><a class="tocitem" href="#Setting-up-a-Bayesian-inference-problems"><span>Setting up a Bayesian inference problems</span></a></li><li><a class="tocitem" href="#Bayesian-inference-(general-setup)"><span>Bayesian inference (general setup)</span></a></li><li><a class="tocitem" href="#Bayesian-inference-with-AdvancedHMC.jl-(NUTS)"><span>Bayesian inference with AdvancedHMC.jl (NUTS)</span></a></li><li><a class="tocitem" href="#Bayesian-inference-with-AdaptiveMCMC.jl"><span>Bayesian inference with AdaptiveMCMC.jl</span></a></li><li><a class="tocitem" href="#Bayesian-inference-with-Pigeons.jl"><span>Bayesian inference with Pigeons.jl</span></a></li></ul></li><li><a class="tocitem" href="../Gradient_hessian_support/">Supported gradient and hessian methods</a></li><li><a class="tocitem" href="../Best_options/">Choosing the best options for a PEtab problem</a></li><li><a class="tocitem" href="../API_choosen/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Bayesian inference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Bayesian inference</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/sebapersson/PEtab.jl/blob/main/docs/src/HMC.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Bayesian-Inference"><a class="docs-heading-anchor" href="#Bayesian-Inference">Bayesian Inference</a><a id="Bayesian-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Inference" title="Permalink"></a></h1><p>When fitting a model with PEtab.jl the unknown model parameters are estimated within a frequentist framework, and the goal is to find the maximum likelihood estimate. When prior knowledge about the parameters is available, Bayesian inference is an alternative approach to fitting the model to data. The aim with Bayesian inference is to infer the posterior distribution of unknown parameters given the data, <span>$p(\theta | y)$</span> by running Markov chain Monte Carlo (MCMC) algorithm that samples from the Posterior.</p><p>PEtab.jl supports Bayesian inference via three packages:</p><ul><li><strong>Adaptive Metropolis Hastings Samplers</strong> available in <a href="https://github.com/mvihola/AdaptiveMCMC.jl">AdaptiveMCMC.jl</a></li><li><strong>Hamiltonian Monte Carlo (HMC) Samplers</strong>: available in <a href="https://github.com/TuringLang/AdvancedHMC.jl">AdvancedHMC.jl</a>. Here the default choice is the NUTS sampler, which is used by <a href="https://github.com/TuringLang/Turing.jl">Turing.jl</a>, and is also the default in Stan. HMC samplers are often more efficient than other methods.</li><li><strong>Adaptive Parallel Tempering Samplers</strong>: available in <a href="https://github.com/Julia-Tempering/Pigeons.jl">Pigeons.jl</a>. Parallel tempering is most suitable for multi-modal (it can jump modes) or non-identifiable posteriors.</li></ul><p>This document covers how to create a <code>PEtabODEProblem</code> with priors, and how to use <a href="https://github.com/mvihola/AdaptiveMCMC.jl">AdaptiveMCMC.jl</a>, <a href="https://github.com/TuringLang/AdvancedHMC.jl">AdvancedHMC.jl</a>, and <a href="https://github.com/Julia-Tempering/Pigeons.jl">Pigeons.jl</a> for Bayesian inference.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>To use the Bayesian inference functionality in PEtab.jl, the Bijectors, LogDensityProblems, and LogDensityProblemsAD packages must be loaded. For parallel tempering Pigeons and MCMCChains must also be loaded.</p></div></div><h2 id="Setting-up-a-Bayesian-inference-problems"><a class="docs-heading-anchor" href="#Setting-up-a-Bayesian-inference-problems">Setting up a Bayesian inference problems</a><a id="Setting-up-a-Bayesian-inference-problems-1"></a><a class="docs-heading-anchor-permalink" href="#Setting-up-a-Bayesian-inference-problems" title="Permalink"></a></h2><p>If the PEtab problem is in the PEtab standard format, priors for model parameters can be defined in the <a href="https://petab.readthedocs.io/en/latest/documentation_data_format.html#parameter-table">Parameter table</a>. Here, we show how to set up priors for a <code>PEtabODEProblem</code> defined directly in Julia, using a simple saturated growth model as an example. First, we create the model and simulate some data:</p><pre><code class="language-julia hljs">using ModelingToolkit, OrdinaryDiffEq, PEtab, Plots

# Dynamic model
@parameters b1, b2
@variables t x(t)
D = Differential(t)
eqs = [
    D(x) ~ b2*(b1 - x)
]
specie_map = [x =&gt; 0]
parameter_map = [b1 =&gt; 1.0, b2 =&gt; 0.2]
@named sys = ODESystem(eqs)

# Simulate data
using Random, Distributions
Random.seed!(1234)
oprob = ODEProblem(sys, specie_map, (0.0, 2.5), parameter_map)
tsave = collect(range(0.0, 2.5, 101))
dist = Normal(0.0, 0.03)
_sol = solve(oprob, Rodas4(), abstol=1e-12, reltol=1e-12, saveat=tsave, tstops=tsave)
# Normal measurement noise with σ = 0.03
obs = _sol[:x] .+ rand(Normal(0.0, 0.03), length(tsave))
plot(_sol.t, obs, seriestype=:scatter, title = &quot;Observed data&quot;)</code></pre><img src="4ca9ca68.svg" alt="Example block output"/><p>Now we can provide the rest of information needed for setting up a <code>PEtabODEProblem</code> (for a starter on setting up a <code>PEtabODEProblem</code> see <a href="../Define_in_julia/#define_in_julia">here</a>):</p><pre><code class="language-julia hljs">using DataFrames
# Measurement data
measurements = DataFrame(
    obs_id=&quot;obs_X&quot;,
    time=_sol.t,
    measurement=obs)
# Observable
@parameters sigma
obs_X = PEtabObservable(x, sigma)
observables = Dict(&quot;obs_X&quot; =&gt; obs_X)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Dict{String, PEtabObservable{Symbolics.Num, Symbolics.Num}} with 1 entry:
  &quot;obs_X&quot; =&gt; PEtabObservable: h = x(t), noise-formula = sigma and normal (Gauss…</code></pre><p>When defining the parameters to infer, we can assign a prior using any continuous distribution available in <a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a>. For instance, we can set the following priors:</p><ul><li><span>$b_1$</span>: Uniform distribution between 0.0 and 5.0; <span>$b_1 \sim \mathcal{U}(0.0, 5.0)$</span>.</li><li><span>$\mathrm{log}_{10}(b_2)$</span> : Uniform distribution between -6.0 and <span>$\mathrm{log}_{10}(5.0)$</span>, <span>$\mathrm{log}_{10}(b_2) \sim \mathcal{U}\big(-6.0, \mathrm{log}_{10}(5.0) \big)$</span>.</li><li><span>$\sigma$</span> : Gamma distribution with shape and rate parameters both set to 1.0, <span>$\sigma \sim \mathcal{G}(1.0, 1.0)$</span>.</li></ul><pre><code class="language-julia hljs">_b1 = PEtabParameter(b1, value=1.0, lb=0.0, ub=5.0, scale=:log10, prior_on_linear_scale=true, prior=Uniform(0.0, 5.0))
_b2 = PEtabParameter(b2, value=0.2, scale=:log10, prior_on_linear_scale=false, prior=Uniform(-6, log10(5.0)))
_sigma = PEtabParameter(sigma, value=0.03, lb=1e-3, ub=1e2, scale=:lin, prior_on_linear_scale=true, prior=Gamma(1.0, 1.0))
parameters_ets = [_b1, _b2, _sigma]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{PEtabParameter}:
 PEtabParameter b1. Estimated on log10-scale with bounds [0.0e+00, 5.0e+00] and prior Distributions.Uniform{Float64}(a=0.0, b=5.0)
 PEtabParameter b2. Estimated on log10-scale with bounds [1.0e-03, 1.0e+03] and prior Distributions.Uniform{Float64}(a=-6.0, b=0.6989700043360189)
 PEtabParameter sigma. Estimated on lin-scale with bounds [1.0e-03, 1.0e+02] and prior Distributions.Gamma{Float64}(α=1.0, θ=1.0)</code></pre><p>When specifying priors in PEtab.jl, it is important to note that parameters are by default estimated on the <span>$\mathrm{log}_{10}$</span> scale (can be changed by <code>scale</code> argument). When <code>prior_on_linear_scale=false</code> the prior applies to this parameter scale (default <span>$\mathrm{log}_{10}$</span>), therefore the prior for <code>b2</code> is on the <span>$\mathrm{log}_{10}$</span> scale. If <code>prior_on_linear_scale=true</code>, the prior is in the linear scale, which in this case holds for <code>b1</code> and <code>sigma</code>. If a prior is not specified, the default prior is a Uniform distribution on the parameter scale, with bounds that correspond to upper and lower bounds specified for the <code>PEtabParameter</code>.</p><p>With the priors defined, we can proceed to create a <code>PEtabODEProblem</code>.</p><pre><code class="language-julia hljs">petab_model = PEtabModel(sys, observables, measurements, parameters_ets; state_map=specie_map, verbose=false)
petab_problem = PEtabODEProblem(petab_model; ode_solver=ODESolver(Rodas5(), abstol=1e-6, reltol=1e-6), verbose=false)</code></pre><h2 id="Bayesian-inference-(general-setup)"><a class="docs-heading-anchor" href="#Bayesian-inference-(general-setup)">Bayesian inference (general setup)</a><a id="Bayesian-inference-(general-setup)-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-inference-(general-setup)" title="Permalink"></a></h2><p>The first step to performing Bayesian inference is to construct a <code>PEtabLogDensity</code>. This structure supports the <a href="https://github.com/tpapp/LogDensityProblems.jl">LogDensityProblems.jl</a> interface, meaning it supports all the necessary methods for performing Bayesian inference. To create it provide the <code>PEtabODEProblem</code> as an argument.</p><pre><code class="language-julia hljs">using Bijectors, LogDensityProblems, LogDensityProblemsAD
target = PEtabLogDensity(petab_problem)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">PEtabLogDensity with 3 parameters to infer.</code></pre><p>When later performing Bayesian inference, the settings for the ODE solver and gradient computations are those in <code>petab_problem</code>. For instance, in this case, we use the default gradient method (<code>ForwardDiff</code>) and simulate the ODE model with the <code>Rodas5</code> ODE solver.</p><p>Inference can now be performed. Although the choice of a starting point for the inference process is crucial, for simplicity we use the parameter vector used for simulating the data (note that the second parameter is on <span>$\mathrm{log}_{10}$</span> scale).</p><pre><code class="language-julia hljs">xpetab = petab_problem.θ_nominalT</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{Float64}:
  0.0
 -0.6989700043360187
  0.03</code></pre><p>Lastly, when conducting Bayesian inference with PEtab.jl, an <strong>important</strong> note is that inference is performed on the prior scale. For instance, if a parameter is set with <code>scale=:log10</code>, but the prior is defined on the linear scale (<code>prior_on_linear_scale=true</code>), then inference is performed on the linear scale. Moreover, Bayesian inference algorithms typically prefer to operate in an unconstrained space, that is a prior such as <span>$b_1 \sim \mathcal{U}(0.0, 5.0)$</span>, where the parameter is bounded is not ideal. To address this, bounded parameters are <a href="https://mc-stan.org/docs/reference-manual/change-of-variables.html">transformed</a> to be unconstrained.</p><p>In summary, for a parameter vector on the PEtab parameter scale (<code>xpetab</code>), for inference we must transform to the prior scale (<code>xprior</code>), and then to the inference scale (<code>xinference</code>). This can be done via:</p><pre><code class="language-julia hljs">xprior = to_prior_scale(petab_problem.θ_nominalT, target)
xinference = target.inference_info.bijectors(xprior)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{Float64}:
 -1.3862943611198906
  1.3329014098421619
 -3.506557897319982</code></pre><div class="admonition is-category-warn"><header class="admonition-header">Warn</header><div class="admonition-body"><p>To get correct inference results, it is important that the starting value is on the transformed parameter scale (as <code>xinference</code> above).</p></div></div><h2 id="Bayesian-inference-with-AdvancedHMC.jl-(NUTS)"><a class="docs-heading-anchor" href="#Bayesian-inference-with-AdvancedHMC.jl-(NUTS)">Bayesian inference with AdvancedHMC.jl (NUTS)</a><a id="Bayesian-inference-with-AdvancedHMC.jl-(NUTS)-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-inference-with-AdvancedHMC.jl-(NUTS)" title="Permalink"></a></h2><p>Given a starting point we can run the NUTS sampler with 2000 samples, and 1000 adaptation steps:</p><pre><code class="language-julia hljs">using AdvancedHMC
# δ=0.8 - acceptance rate (default in Stan)
sampler = NUTS(0.8)
Random.seed!(1234)
res = sample(target, sampler,
             2000;
             nadapts = 1000,
             initial_params = xinference,
             drop_warmup=true,
             progress=false)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">[ Info: Found initial step size 0.00625</code></pre><p>Any other algorithm found in AdvancedHMC.jl <a href="https://github.com/TuringLang/AdvancedHMC.jl">documentation</a> can also be used.</p><p>To put the output into an easy to interact with format, we can convert it to a <a href="https://github.com/TuringLang/MCMCChains.jl">MCMCChains</a></p><pre><code class="language-julia hljs">using MCMCChains
chain_hmc = PEtab.to_chains(res, target)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chains MCMC chain (2000×3×1 Array{Float64, 3}):

Iterations        = 1:1:2000
Number of chains  = 1
Samples per chain = 2000
parameters        = b1, b2, sigma

Summary Statistics
  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯
      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯

          b1    1.8903    0.9385    0.0663   229.4717   265.8819    1.0004     ⋯
          b2   -0.9551    0.2144    0.0145   230.3404   273.5797    1.0004     ⋯
       sigma    0.0316    0.0024    0.0001   850.8247   865.1639    1.0005     ⋯
                                                                1 column omitted

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5%
      Symbol   Float64   Float64   Float64   Float64   Float64

          b1    0.8427    1.2146    1.5631    2.3041    4.3950
          b2   -1.4008   -1.1038   -0.9175   -0.7929   -0.6042
       sigma    0.0274    0.0299    0.0313    0.0331    0.0364
</code></pre><p>which we can also plot:</p><pre><code class="language-julia hljs">using Plots, StatsPlots
plot(chain_hmc)</code></pre><img src="5acdc4e7.svg" alt="Example block output"/><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>When converting the output to a <code>MCMCChains</code> the parameters are transformed to the prior-scale (inference scale).</p></div></div><h2 id="Bayesian-inference-with-AdaptiveMCMC.jl"><a class="docs-heading-anchor" href="#Bayesian-inference-with-AdaptiveMCMC.jl">Bayesian inference with AdaptiveMCMC.jl</a><a id="Bayesian-inference-with-AdaptiveMCMC.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-inference-with-AdaptiveMCMC.jl" title="Permalink"></a></h2><p>Given a starting point we can run the robust adaptive MCMC sampler with <span>$200 \, 000$</span> by:</p><pre><code class="language-julia hljs">using AdaptiveMCMC
Random.seed!(123)
# target.logtarget = posterior logdensity
res = adaptive_rwm(xinference, target.logtarget, 200000; progress=false)</code></pre><p>and we can convert the output to a <code>MCMCChains</code></p><pre><code class="language-julia hljs">chain_adapt = to_chains(res, target)
plot(chain_adapt)</code></pre><img src="e01a2c76.svg" alt="Example block output"/><p>Any other algorithm found in AdaptiveMCMC.jl <a href="https://github.com/mvihola/AdaptiveMCMC.jl">documentation</a> can also be used.</p><h2 id="Bayesian-inference-with-Pigeons.jl"><a class="docs-heading-anchor" href="#Bayesian-inference-with-Pigeons.jl">Bayesian inference with Pigeons.jl</a><a id="Bayesian-inference-with-Pigeons.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-inference-with-Pigeons.jl" title="Permalink"></a></h2><p>When using a parallel tempering algorithm an easy to sample from reference distribution that covers a wide range of parameter space is needed to, for example, jump between modes. For a <code>PEtabODEProblem</code>, this reference is the prior distribution, and to set it up do:</p><pre><code class="language-julia hljs">reference = PEtabPigeonReference(petab_problem)</code></pre><p>Given a starting point we can now take <span>$2^{10}$</span> adaptive parallel tempering samples by:</p><pre><code class="language-julia hljs">using Pigeons
Random.seed!(123)
target.initial_value .= xinference
pt = pigeons(target = target,
             reference = reference,
             n_rounds=10, # 2^10 samples
             record = [traces; record_default()])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">──────────────────────────────────────────────────────────────────────────────────────────────────
  scans        Λ        time(s)    allc(B)  log(Z₁/Z₀)   min(α)     mean(α)    min(αₑ)   mean(αₑ)
────────── ────────── ────────── ────────── ────────── ────────── ────────── ────────── ──────────
        2       2.25       10.2   2.11e+09        189   8.44e-14       0.75          1          1
        4       2.75      0.874   9.34e+08        197      0.013      0.695          1          1
        8          3       1.46   1.67e+09        198      0.338      0.667          1          1
       16       2.89       2.96    3.4e+09        195      0.222      0.679          1          1
       32       3.54       5.51   6.74e+09        196      0.356      0.606          1          1
       64       3.79       10.8   1.34e+10        194      0.429      0.579          1          1
      128       3.46       21.8   2.69e+10        196      0.478      0.615          1          1
      256       3.75       42.9   5.32e+10        195      0.542      0.584          1          1
      512        3.7       86.1   1.07e+11        195      0.564      0.589          1          1
 1.02e+03       3.72        174   2.14e+11        195       0.55      0.587          1          1
──────────────────────────────────────────────────────────────────────────────────────────────────</code></pre><p>and we can convert the output to a <code>MCMCChains</code></p><pre><code class="language-julia hljs">pt_chain = to_chains(pt, target)
plot(pt_chain)</code></pre><img src="9e66272c.svg" alt="Example block output"/><p>Here we used the default <code>SliceSampler</code> as local explorer. Alternative explorers, such as an adaptive MALA, can be used instead by setting <code>explorer=AutoMALA()</code> in the <code>pigeons</code> function call. More information can be found in Pigeons.jl <a href="https://pigeons.run/dev/">documentation</a>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../optimisation_output_plotting/">« Plots evaluating parameter estimation</a><a class="docs-footer-nextpage" href="../Gradient_hessian_support/">Supported gradient and hessian methods »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Wednesday 13 March 2024 13:30">Wednesday 13 March 2024</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
