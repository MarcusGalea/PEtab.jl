# [Plots evaluating parameter estimation](@id optimisation_output_plotting)

After fitting a model's parameter to data, it is prudent to evaluate the results. The simplest approach is to simulate the model using the optimally fitting parameter set, visually inspecting its goodness of fit. However, there exist several techniques attempting to deduce whenever:
- There exist unfound parameter sets that yield better fits than what was found (suggesting a local minimum was reached).
- There exist additional parameter sets yielding equally good fits to what was found (suggesting an *identifiability* problem).

This section will demonstrate various plots implemented in PEtab that can be used to perform this kind of analysis. These plots can be generated by calling `plot` on the output of `calibrate_model` (a `PEtabOptimisationResult` structure) or `calibrate_model_multistart` (a `PEtabMultistartOptimisationResult` structure). In addition, an optional argument (`plot_type`) allows the selection of plot type.

First, we load a multi start optimisation result:
```julia
using PEtab, Plots
petab_ms_res = PEtabMultistartOptimisationResult(joinpath(@__DIR__, "assets", "optimisation_results", "boehm"))
nothing # hide
```
next, we will, throughout the following sections, demonstrate the available types of plots using `petab_ms_res` as input.

## Objective function evaluations

The objective function evaluations plot is accessed through the `plot_type=objective` argument. It is valid both for single start and multi start optimisation results. It plots, for each iteration of the optimisation process, the achieved objective value.

For single start optimisation results, a single trajectory of dots is plotted. For a multi start optimisation result, each run correspond to a separate trajectory of dots.
```julia
plot(petab_ms_res; plot_type=objective)
```
Here, the runs are separated my different colours. First, a clustering process is carried out, attempting to identify runs converging to the same local minimum (the clustering, including how to customise it, is described [here](@ref optimisation_output_plotting_multirun_indexing)). Next, runs from the same cluster is assigned the same colour. A similar scheme is used for all plots involving multi start runs. When a large number of runs are carried out, it is also possible to select which one to include in the plot (by default the `10` best ones are used, however, this can be [customised](@ref optimisation_output_plotting_multirun_indexing)).

Sometimes, the objective function is unable to successfully simulate the model for a specific parameter set (this indicates a very poor fit). If such evaluations happen, they are marked with crosses (rather than circles) in these plots. 

## Best objective function evaluations

The best objective function evaluations plot is accessed through the `plot_type=best_objective` argument. It is valid both for single start and multi start optimisation results. This function is very similar to the objective function evaluation one, however, it instead plots the *best value reached so far* in the process (and is thus a decreasing function.) The best objective function evaluations plot is the default plot type for single start optimisation results.
```julia
plot(petab_ms_res; plot_type=best_objective)
```

## Waterfall plots

The waterfall plot is accessed through the `plot_type=waterfall` argument. It is only valid for multi start optimisation results. It plots all runs' final objective values, sorted from best to worst (local minimums can typically be identified as plateaus in the plot). The waterfall plot is the default plot type for multi start optimisation results.
```julia
plot(petab_ms_res; plot_type=waterfall)
```

## Parallel coordinates plots

The parallel coordinates plot is accessed through the `plot_type=parallel_coordinates` argument. It is only valid for multi start optimisation results. In it, each run corresponds to a line, drawn along its parameter values in vector of fitted parameters (starting with the first, and ending in the last, parameter). The parameter values are normalised (so that `0` corresponds to the minimum value encountered for that parameter, and `1` the maximum values). If runs in the same cluster typically share similar paths across the parameter values, this suggests that they have converged to the same local minimum. If, for one parameters, runs in the same cluster have widely different values, it suggests that that parameter is unidentifiable.
```julia
plot(petab_ms_res; plot_type=parallel_coordinates)
```

## Runtime evaluation plots

The runtime evaluation plot is accessed through the `plot_type=runtime_eval` argument. It is only valid for multi start optimisation results. It is a scatter plot, showing for each run, how its final objective value depends on the runtime.
```julia
plot(petab_ms_res; plot_type=runtime_eval)
```


## [Multi start run clustering](@id optimisation_output_plotting_multirun_clustering)

When using the `calibrate_model_multistart` function to fit a parameter set, several independent runs are performed. In all the previous plots, a clustering function is applied to identify runs that likely have converged to the same local minimum. Runs in the same cluster are given the same colour in the plots, allowing the cluster to be identified. By default, the `objective_value_clustering` function is used for this (roughly, it clusters runs together if their objective function evaluates to values within `0.1` of each other). It is possible for the user to define their own clustering function, and supply it to the `plot` command through the `clustering_function` argument. The clustering function should take a `Vector{PEtabOptimisationResult}` input, and return an identical sized `Vector{Int64}` (for each index giving an integer corresponding to that run's cluster).


## [Sub-selecting runs to plot](@id optimisation_output_plotting_multirun_indexing)
When plotting a multi start output using the `:objective`, `:best_objective`, or `:parallel_coordinates` plot types, if the number of runs are large, it can sometimes be hard to distinguish information from the plot. Hence, for these plot types, only the `10` runs with the best final objective values are plotted. This can be modified through the `best_idxs_n` optional argument. This is an `Int64`, what number of runs to add to the plot (starting with the best one). Alternatively, the `idxs` optional argument can be used to give the indexes of the runs to plot. 

For the `:waterfall`, `:runtime_eval` plot types, by default all runs are plotted. However, if desired, the `best_idxs_n` and `idxs` arguments can be used for these plot types as well.

